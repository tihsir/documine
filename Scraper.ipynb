{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee57646-1c33-4352-bbb7-9227f50c0c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: wikipedia in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install wikipedia\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d1704d-c44c-447e-87db-292370e2cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib import request\n",
    "import urllib.request as ur\n",
    "import requests\n",
    "import wikipedia\n",
    "import random\n",
    "from constants import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "all_titles = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5d707-a8c2-4c24-89ec-aee4241976e3",
   "metadata": {},
   "source": [
    "# Recursively get relevant wikipedia page links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711cf61a-5171-4635-9c76-5ec437613bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapes all the relevant titles.\n",
    "def scrapeLinks(url, depth, num_links_per_page=10):\n",
    "\n",
    "    # url filtering to remove wiki assets:\n",
    "    if (len(url.split(\"/\")) != 5):\n",
    "        return \n",
    "    last_path = url.split(\"/\")[4]\n",
    "    if last_path.startswith(\"Wikipedia:\") \\\n",
    "        or last_path.startswith(\"Category:\")\\\n",
    "        or last_path.startswith(\"Template:\") \\\n",
    "        or last_path.startswith(\"File:\") \\\n",
    "        or last_path.startswith(\"Help:\") \\\n",
    "        or last_path.startswith(\"Special:\"):\n",
    "        return\n",
    "\n",
    "    global all_titles\n",
    "    all_titles.append(url)\n",
    "    \n",
    "    # end condition\n",
    "    if (depth == 0):\n",
    "        return\n",
    "    \n",
    "    response = requests.get(\n",
    "        url=url,\n",
    "    )\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find(id=\"firstHeading\")\n",
    "    allLinks = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "    random.shuffle(allLinks)\n",
    "\n",
    "    # limit to top n links\n",
    "    allLinks = allLinks[:num_links_per_page]\n",
    "    \n",
    "    # Recursively scrape links\n",
    "    for link in allLinks:\n",
    "        try: \n",
    "            if link['href'].find(\"/wiki/\") == -1:\n",
    "                continue\n",
    "                \n",
    "            # Use this link to scrape\n",
    "            scrapeLinks(\"https://en.wikipedia.org\" + link['href'], depth - 1)\n",
    "        except:\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e72d93-48fc-4361-89b7-cf4c69ae5c7a",
   "metadata": {},
   "source": [
    "# Preprocess each of the wikipedia pages and remove headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4d503-4fad-4537-9f3d-7f6addc582a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeWikiLinksToCSV(search_term):\n",
    "    \n",
    "    global all_titles\n",
    "    \n",
    "    # Search for similar wiki titles and try all relevant routes:\n",
    "    relevant_routes = wikipedia.search(search_term)\n",
    "    for i in relevant_routes:\n",
    "        try:\n",
    "            # construct url:\n",
    "            start_url = wikipedia.page(i).url\n",
    "            \n",
    "            # scrapelinks and accumulate in global all_titles\n",
    "            scrapeLinks(start_url, SEARCH_DEPTH, NUM_LINKS_PER_PAGE)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(columns=[\"url\", \"title\", \"content\"])\n",
    "    wikipedia.set_lang(\"en\")\n",
    "    for link in all_titles:\n",
    "        # get basepath:\n",
    "        title = link.split('/')[-1]\n",
    "        try:\n",
    "\n",
    "            # only get the first NUM_SENTENCES_FROM_WIKI amount of sentences\n",
    "            content = wikipedia.summary(title, sentences=NUM_SENTENCES_FROM_WIKI)\n",
    "                \n",
    "            # preprocess content to remove \"==== xxx ====\"\n",
    "            content = re.sub(r'==.*?==', '', content)  \n",
    "            \n",
    "            # add to dataframe\n",
    "            df.loc[len(df)] = [link, title, content]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # save as csv.\n",
    "    df.to_csv('scraped.csv', index=False)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aad24e-c682-4352-a64c-c83aa0879306",
   "metadata": {},
   "source": [
    "# Run Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "408e5667-4787-4c62-9984-964d871b31fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/chromerag/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/opt/miniconda3/envs/chromerag/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/opt/miniconda3/envs/chromerag/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/miniconda3/envs/chromerag/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "df = scrapeWikiLinksToCSV(INPUT_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d67cfdd-33b5-4463-8aec-ef59f77dd767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Web_scraping</td>\n",
       "      <td>Web_scraping</td>\n",
       "      <td>Web scraping, web harvesting, or web data extr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Document_Object_...</td>\n",
       "      <td>Document_Object_Model</td>\n",
       "      <td>The Document Object Model (DOM) is a cross-pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://en.wikipedia.org/wiki/List_of_unit_tes...</td>\n",
       "      <td>List_of_unit_testing_frameworks#JavaScript</td>\n",
       "      <td>This is a list of notable test automation fram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Blink_(browser_e...</td>\n",
       "      <td>Blink_(browser_engine)</td>\n",
       "      <td>Blink is a browser engine developed as part of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Serialization</td>\n",
       "      <td>Serialization</td>\n",
       "      <td>In computing, serialization (or serialisation,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0         https://en.wikipedia.org/wiki/Web_scraping   \n",
       "1  https://en.wikipedia.org/wiki/Document_Object_...   \n",
       "2  https://en.wikipedia.org/wiki/List_of_unit_tes...   \n",
       "3  https://en.wikipedia.org/wiki/Blink_(browser_e...   \n",
       "4        https://en.wikipedia.org/wiki/Serialization   \n",
       "\n",
       "                                        title  \\\n",
       "0                                Web_scraping   \n",
       "1                       Document_Object_Model   \n",
       "2  List_of_unit_testing_frameworks#JavaScript   \n",
       "3                      Blink_(browser_engine)   \n",
       "4                               Serialization   \n",
       "\n",
       "                                             content  \n",
       "0  Web scraping, web harvesting, or web data extr...  \n",
       "1  The Document Object Model (DOM) is a cross-pla...  \n",
       "2  This is a list of notable test automation fram...  \n",
       "3  Blink is a browser engine developed as part of...  \n",
       "4  In computing, serialization (or serialisation,...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297d28f-4359-4dfd-a64a-326844e87dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc969b0-5687-47ab-a57b-e0fbb266c0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85cab4c-c70a-410a-a5ad-f7b18184ff93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb79f7-9af4-4e11-8179-4411db4d1086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
